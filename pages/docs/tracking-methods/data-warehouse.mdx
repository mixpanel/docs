import { Callout, Tabs } from 'nextra/components'

import ExtendedTabs from '../../../components/ExtendedTabs/ExtendedTabs';
import { dwhItems, mirrorItems } from '../../../utils/constants';

# Warehouse Connectors: Sync data from your data warehouse into Mixpanel

<Callout type="info">
Warehouse Connectors are an enterprise add-on. [Reach out](https://mixpanel.com/contact-us/sales/) to our team to have it enabled on your account.
</Callout>

With Warehouse Connectors you can sync data from data warehouses like Snowflake, BigQuery, Databricks, and Redshift to Mixpanel. By unifying business data with product usage events, you can answer many more questions in Mixpanel:
* What percentage of our Enterprise revenue uses the features we shipped last year?
* Did our app redesign reduce support tickets?
* Which account demographics have the best retention?
* We spent $50,000 on a marketing campaign, did the users we acquire stick around a month later?

Mixpanel's [Mirror](#mirror) sync mode keeps the data in Mixpanel fully in sync with any changes that occur in the warehouse including updating historical events that are deleted or modified in your warehouse.

In this guide, we'll walk through how to set up Warehouse Connectors. The integration is completely codeless, but you will need someone with access to your DWH to help with the initial set up.

## Getting Started

### Step 1: Connect a warehouse

Navigate to [Project Settings → Warehouse Sources](https://mixpanel.com/report/settings/%23project%2F%24project_id%24%2Fwarehousesources/). Select your warehouse and follow the instructions to connect it. Note: you only need to do this once.

<ExtendedTabs urlParam="dwh" urlToItemsMap={dwhItems}>

<Tabs.Tab>
    <div style={{position: 'relative', paddingBottom: '64.90384615384616%', height: 0}}>
        <iframe src="https://www.loom.com/embed/04f4ea75310744cdab477e1b47684db3?sid=3728258e-b860-431f-bdf2-cff508b2b19e"
            frameBorder="0"
            webkitallowfullscreen="true" mozallowfullscreen="true" allowFullScreen
            style={{position: 'absolute', 'top': 0, 'left': 0, 'width': '100%', 'height': '100%'}}>
        </iframe>
    </div>

        The BigQuery connector works by giving a Mixpanel-managed service account permission to read from BigQuery in your GCP project. You will need:
        - Your GCP Project ID, which you can find in the URL of Google Cloud Console (`https://console.cloud.google.com/bigquery?project=YOUR_GCP_PROJECT`).
        - Your unique Mixpanel service account ID, which is is generated the first time you create a BigQuery connection in the Mixpanel UI
          (e.g. `project-?????@mixpanel-warehouse-1.iam.gserviceaccount.com`).
        - A new, empty `mixpanel` dataset in your BigQuery instance (if you are using [Mirror](#mirror)).
        ```jsx
          CREATE SCHEMA `<gcp-project>`.`mixpanel`
            OPTIONS (
              description = 'Mixpanel connector staging dataset',
              location = '<same-as-the-tables-to-be-synced>',
            );
        ```

        Grant the the Mixpanel service the following permissions:
        - `roles/bigquery.jobUser` - Allows Mixpanel to run BigQuery jobs to unload data.
          ```jsx
          gcloud projects add-iam-policy-binding --member serviceAccount:<mixpanel-service-account> --role roles/bigquery.jobUser
          ```
        - `roles/bigquery.dataViewer` on the datasets and/or tables to sync. Gives Mixpanel read-only access to the datasets.
          ```jsx
          GRANT `roles/bigquery.dataViewer`
            ON SCHEMA `<gcp-project>`.`<dataset-to-be-synced>`
            TO "<mixpanel-service-account>"
          ```
        - `roles/bigquery.dataOwner` on the `mixpanel` dataset. Gives Mixpanel read-write access to the `mixpanel` dataset.
          ```jsx
          GRANT `roles/bigquery.dataOwner`
            ON SCHEMA `<gcp-project>`.`mixpanel`
            TO "<mixpanel-service-account>"
          ```

</Tabs.Tab>
<Tabs.Tab>
    <div style={{position: 'relative', paddingBottom: '64.90384615384616%', height: 0}}>
        <iframe src="https://www.loom.com/embed/5dbaf37d9308404393417043551aed55"
            frameBorder="0"
            webkitallowfullscreen="true" mozallowfullscreen="true" allowFullScreen
            style={{position: 'absolute', 'top': 0, 'left': 0, 'width': '100%', 'height': '100%'}}>
        </iframe>
    </div>

    To connect to Snowflake you will need:
    - Your [Snowflake account identifier](https://docs.snowflake.com/user-guide/admin-account-identifier), which you can find in the
      URL of your Snowflake account (`https://YOUR_ACCOUNT_NAME.snowflakecomputing.com/`).
    - A dedicated Mixpanel user account and role. The user account can use either key-pair or password authentication.
      If using key-pair authentication Mixpanel will generate a secure key-pair during the connection process. The public
      key will be provided during the setup process and the private key will be encrypted and stored securely.
      ```jsx
      CREATE ROLE MIXPANEL_ROLE;
      # one of
      CREATE USER MIXPANEL PASSWORD='?????' DEFAULT_ROLE=MIXPANEL_ROLE;
      CREATE USER MIXPANEL RSA_PUBLIC_KEY='<mixpanel-provided-key>' DEFAULT_ROLE=MIXPANEL_IMPORT_ROLE;
      # then
      GRANT ROLE MIXPANEL_ROLE TO USER MIXPANEL;
      ```
    - A Snowflake [WAREHOUSE](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) Mixpanel will use to unload data.
      We recommend creating a dedicated warehouse for Mixpanel to avoid impacting other workloads.
      ```jsx
      CREATE WAREHOUSE MIXPANEL_WAREHOUSE WITH
        WAREHOUSE_SIZE = XSMALL # consider increasing for larger datasets
        AUTO_SUSPEND = 60
        AUTO_RESUME = TRUE
        INITIALLY_SUSPENDED = TRUE;
      GRANT USAGE ON MIXPANEL_WAREHOUSE TO ROLE MIXPANEL_ROLE;
      GRANT OPERATE ON MIXPANEL_WAREHOUSE TO ROLE MIXPANEL_ROLE;
      GRANT MONITOR ON MIXPANEL_WAREHOUSE TO ROLE MIXPANEL_ROLE;
      ```
    - A Snowflake [STORAGE INTEGRATION](https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration) Mixpanel will
      use to unload data. This integration can optionally restrict STORAGE_ALLOWED_LOCATIONS to the unique Mixpanel-managed bucket
      created to receive data for this connection.
      ```jsx
      CREATE STORAGE INTEGRATION MIXPANEL_STORAGE_INTEGRATION
        TYPE = EXTERNAL_STAGE
        STORAGE_PROVIDER = 'GCS'
        ENABLED = TRUE
        STORAGE_ALLOWED_LOCATIONS = ("<mixpanel-provided-bucket-name>"); # optional
      GRANT USAGE ON INTEGRATION MIXPANEL_STORAGE_INTEGRATION TO MIXPANEL_ROLE;
      ```
    - A new, empty `MIXPANEL` [SCHEMA](https://docs.snowflake.com/en/sql-reference/sql/create-schema) the Mixpanel user has the `USAGE`
      and `CREATE STREAM` permission for (if you are using [Mirror](#mirror)).
      ```jsx
      CREATE SCHEMA <database>.MIXPANEL;
      GRANT USAGE ON DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT USAGE ON SCHEMA <database>.MIXPANEL TO ROLE MIXPANEL_ROLE;
      GRANT CREATE STREAM ON SCHEMA <database>.MIXPANEL TO ROLE MIXPANEL_ROLE;
      ```
    - The Mixpanel user needs the `USAGE` and `SELECT` permissions to have read-only access to any tables and views you plan to sync.
      Adjust this example to fine tune permissions.
      ```jsx
      GRANT USAGE ON DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT USAGE ON ALL SCHEMAS IN DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT USAGE ON FUTURE SCHEMAS IN DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT SELECT ON ALL TABLES IN DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT SELECT ON FUTURE TABLES IN DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT SELECT ON ALL VIEWS IN DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT SELECT ON FUTURE VIEWS IN DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE <database> TO ROLE MIXPANEL_ROLE;
      GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE <database> TO ROLE MIXPANEL_ROLE;
      ```

    **IP Allowed List**

    If you are using [Snowflake Network policy](https://docs.snowflake.com/en/user-guide/network-policies) to restrict access to your instance, you might need to add the following IP addresses to the allowed list.

    ```jsx
    34.31.112.201
    34.147.68.192
    35.184.21.33
    35.225.176.74
    35.204.164.122
    35.204.177.251
    ```

</Tabs.Tab>
<Tabs.Tab>
    <div style={{position: 'relative', paddingBottom: '64.90384615384616%', height: 0}}>
    <iframe src="https://www.loom.com/embed/56a21b31e75342498317cbb6f31285eb"
        frameBorder="0"
        webkitallowfullscreen="true" mozallowfullscreen="true" allowFullScreen
        style={{position: 'absolute', 'top': 0, 'left': 0, 'width': '100%', 'height': '100%'}}>
    </iframe>
</div>

<Callout type="info">
  The connection to Databricks only supports connecting directly to clusters. Connecting directly to jobs or SQL warehouses is not supported. The account used for the connection should have `can attach to` permissions to the cluster (and `can restart` if it auto-suspends), and `can use` to the warehouse the cluster leverages.
</Callout>

Complete the following steps to get your Databricks connector up and running:

1. Navigate to **Project Settings**, then select **Warehouse Sources**.
2. Click on `+ Add Connection` and select **Databricks**.
3. You should see a new page to create your databricks connector. In the first view, fill out the following fields before clicking  `Create Source`:
    - ** Server Hostname ** - This is the hostname of your Databricks cluster. This can be found in your workspace URL, or by navigating to [JDBC/ODBC connection settings](https://docs.databricks.com/en/integrations/jdbc-odbc-bi.html#step)
    - ** HTTP Path ** - This is the HTTP path of the cluster you would like to connect to. This can be found in your cluster [JDBC/ODBC connection settings](https://docs.databricks.com/en/integrations/jdbc-odbc-bi.html#step)
    - ** Access Token ** - This is the Personal access token used to authenticate with your Databricks cluster. Here are the instructions on [how to create an access token](https://docs.databricks.com/en/dev-tools/auth.html#databricks-personal-access-tokens-for-workspace-users)
4. In the second view, you should see that the credentials are validated and a source is added.

**Using Service Principals**
If a service principal is used, you'll want to follow these steps:
1. [Create the service principal](https://docs.databricks.com/en/admin/users-groups/service-principals.html#manage-service-principals-in-your-account) if it doesn't exist.
2. Create a token for the service principal. You can do so through the Databricks cli:
```bash
databricks configure --token
databricks token-management create-obo-token <application_id_of_service_principal> 31536000 --comment "Mixpanel warehouse connector service principal token"
```
3. Copy the token generated for later use
4. Give the service principal `Can Attach` permissions to the cluster (in Cluster > Permissions)
    - Note the cluster needs to be a shared compute resource and not a single user cluster (unless the service principal created the cluster as well)
5. Give the service principal access to the catalogs you want to read in Mixpanel
6. When table access control is enabled in the account, the service principal also requires access to the files that will be read. You can add access like:
```bash
GRANT SELECT ON ANY FILE TO <application_id_of_service_principal>
GRANT MODIFY ON ANY FILE TO <application_id_of_service_principal>
```
7. During the setup processing Mixpanel, you can now use the token from the service principal when setting up the connection


**IP Allowed List**

If you are using [IP Access List](https://docs.databricks.com/en/security/network/front-end/ip-access-list.html) to restrict access to your instance, you might need to add the following IP addresses to the allowed list.

```jsx
34.31.112.201
34.147.68.192
35.184.21.33
35.225.176.74
35.204.164.122
35.204.177.251
```

</Tabs.Tab>
<Tabs.Tab>

<div style={{position: 'relative', paddingBottom: '64.90384615384616%', height: 0}}>
    <iframe src="https://www.loom.com/embed/76f4658dd850457e9634c706ee0d9430"
        frameBorder="0"
        webkitallowfullscreen="true" mozallowfullscreen="true" allowFullScreen
        style={{position: 'absolute', 'top': 0, 'left': 0, 'width': '100%', 'height': '100%'}}>
    </iframe>
</div>

Complete the following steps to get your Redshift connector up and running:

1. Navigate to **Project Settings**, then select **Warehouse Sources**.
2. Click on `+ Add Connection` and select **Redshift**.
3. You should see a new page to create your Redshift connector. In the first view, fill out the following fields before clicking  `Next`:
    - **AWS Account ID**  - This is the AWS account ID that can be found in the dropdown in the AWS Redshift Console.
    - **Cluster ID**  - This is the name of the Redshift cluster.
4. In the second view, you will need to add the following.
    - **AWS Region** - The region code in which your Redshift cluster resides.
    - **S3 Staging Bucket** - This is the name of S3 staging bucket you need to create. We'll use it to extract data from your Redshift tables before importing the data into Mixpanel.
    - Copy the command generated below in the AWS CLI to create the S3 Staging Bucket with the name you specified.
    - **Database Name** - Input the name of the Database where the tables you want to import are stored.
    - (Optional) **Policy Name** - This is an optional name for the policy, which contains role permissions that you need to grant the Mixpanel Service Account. After inputting a policy name, you can either copy paste the JSON in the AWS UI or copy paste the inline command line version that we generate and run in the AWS CLI.
    - **Role Name** - Input the name of the role. After inputting a role name, you can either copy paste the JSON in the AWS UI or copy paste the inline command line version that we generate and run in the AWS CLI. Running this command grants the Mixpanel Service Account the necessary permissions to read and export data from your Redshift tables.
    - Finally, attach the policy you created to the role you created by copy pasting the command in the AWS CLI.
    - Then, click Create Source.
5. In the third view, you should see a confirmation that your source was created. To establish the source connection, we need to ping your Redshift instance to actually create the service account user.
    - **Grant Access to Schema** - Enter the name of the schema you want to grant Mixpanel access to.
    - Copy the command generated and run in your Redshift worksheet. Once that command is run successfully, the connection will be established and you will be able to send data from Redshift tables to Mixpanel.

### IP Allowed List
If you are using [AWS PrivateLink](https://docs.aws.amazon.com/redshift/latest/mgmt/security-private-link.html) to restrict access to your instance, you might need to add the following IP addresses to the allowed list.

```jsx
34.31.112.201
34.147.68.192
35.184.21.33
35.225.176.74
35.204.164.122
35.204.177.251
```
</Tabs.Tab>
</ExtendedTabs>

---

### Step 2: Load a warehouse table

Navigate to [Project Settings → Warehouse Data](https://mixpanel.com/report/settings/%23project%2F%24project_id%24%2Fwarehousedata/) and click +Event Table.

Select a table or view representing an event from your warehouse and tell Mixpanel about the table. Once satisfied with the preview, click run and we’ll establish the sync. The initial load may take a few minutes depending on the size of the table, we show you progress as it’s happening.

🎉 Congrats, you’ve loaded your first warehouse table into Mixpanel! From this point onward, the table will be kept in sync with Mixpanel. You can now use this event throughout Mixpanel’s interface.

## Table Types

Mixpanel’s [Data Model](/docs/how-it-works/concepts) consists of 4 types: Events, User Profiles, Group Profiles, and Lookup Tables. Each have properties, which are arbitrary JSON. Warehouse Connectors lets you turn any table or view in your warehouse into one of these 4 types of tables, provided they match the required schema.

### Events

An event is something that happens at a point in time. It’s akin to a “fact” in dimensional modeling or a log in a database. Events have properties, which describe the event. Learn more about Events [here](/docs/data-structure/events-and-properties).

Here’s an example table that illustrates what can be loaded as events in Mixpanel. The most important fields are the timestamp (when) and the user id (who) — everything else is optional.

| Timestamp | User ID | Item | Brand | Amount | Type |
| --- | --- | --- | --- | --- | --- |
| 2024-01-04 11:12:00 | alice@example.com | shoes | nike | 99.23 | in-store |
| 2024-01-12 11:12:00 | bob@example.com | socks | adidas | 4.56 | online |

Here are more details about the schema we expect for events:

| Column | Required | Type | Description |
| --- | --- | --- | --- |
| Event Name | Yes | String | The name of the event. Eg: Purchase Completed or Support Ticket Filed. Note: you can specify this value statically, it doesn’t need to be a column in the table. |
| Time | Yes | Timestamp | The time at which the event occurred. |
| User ID | No | String or Integer | The unique identifier of the user who performed the event. Eg: 12345 or grace@example.com. |
| Device ID | No | String or Integer | An identifier for anonymous users, useful for tracking pre-login data. Learn more [here](/docs/tracking-methods/id-management/identifying-users) |
| JSON Properties | No | JSON or Object | A field that contains key-value properties in JSON format. If provided, Mixpanel will flatten this field out into properties. |
| All other columns | No | Any | These can be anything. Mixpanel will auto-detect these columns and attach them to the event as properties. |

### User Profiles

A User Profile is a table that describes your users. It’s akin to a “dimension” in dimensional modeling or a relational table in a database. Learn more about User Profiles [here](/docs/data-structure/user-profiles).

Here’s an example table that illustrates what can be loaded as user profiles in Mixpanel. The only important column is the User ID, which is the primary key of the table.

| User ID | Email | Name | Subscription Tier |
| --- | --- | --- | --- |
| 12345 | grace@example.com | Grace Hopper | Pro |
| 45678 | bob@example.com | Bob Noyce | Free |

While Profiles typically only store the state of a user *as of now*, Profile History enables storing the state of a user *over time*. 

#### Setup 
When creating a User Profile sync, set the Table Type to “History Table”. We expect tables to be modeled as a SCD (Slowly Changing Dimensions) Type 2 table. You will need to supply a Start Time column in the sync configuration. Mixpanel will infer a row's end time if a new row with a more recent start time for the same user is detected. 

Supported warehouses: Snowflake, Big Query, Databricks 

Source table requirements: 
- The source table for user/group history is expected to be modeled as an SCD (Slowly Changing Dimension) Type 2 table. This means that the table must maintain all the history over time that you want to use for analysis.
- History tables are supported only with mirror sync mode. Follow these [docs](https://docs.mixpanel.com/docs/tracking-methods/data-warehouse#mirror) to setup your source table to be mirror-compatible.
- The table should have a Timestamp/Date type column signifying the time that the properties on the row become active. This column will need to be supplied as `Start Time`  in the sync configuration.
- The following data types are NOT supported:
    - Lists (eg Snowflake’s ARRAY)
    - Objects (eg Snowflake’s OBJECT)

### Group Profiles

A Group Profile is a table that describes an entity (most often an Account, if you’re a B2B company). They are functionally identical to User Profiles, just used for other non-User entities. Group Profiles are only available if you have the Group Analytics add-on. Learn more about Group Analytics [here](/docs/data-structure/advanced/group-analytics).

Here’s an example table that illustrates what can be loaded as group profiles in Mixpanel. The only important column is the Group Key, which is the primary key of the table.

| Group Key | Name | Domain | ARR | Subscription Tier |
| --- | --- | --- | --- | --- |
| 12345 | Notion | notion.so | 45000 | Enterprise |
| 45678 | Linear | linear.so | 2000 | Pro |

Group Profile History value and setup is similar to the User Profile History section elaborated above

### Lookup Tables

A Lookup Table is useful for enriching Mixpanel properties (e.g. content, skus, currencies) with additional metadata. Learn more about Lookup Tables [here](/docs/data-structure/lookup-tables). Do note the limits of lookup tables indicated [here](/docs/data-structure/lookup-tables#when-shouldnt--i-use-lookup-tables).

Here’s an example table that illustrates what can be loaded as lookup table in Mixpanel. The only important column is the ID, which is the primary key of the table that is eventually mapped to a Mixpanel property

| ID | Song Name | Artist | Genre |
| --- | --- | --- | --- |
| 12345 | One Dance | Drake | Pop |
| 45678 | Voyager | Daft Punk | Electronic |

## Sync Modes

Warehouse Connectors regularly check warehouse tables for changes to load into Mixpanel. The Sync Mode determines
which changes Mixpanel will reflect.

- **Mirror** will keep Mixpanel perfectly in sync with the data in the warehouse. This includes syncing new data,
  modifying historical data, and deleting data that has been removed from the warehouse. **Mirror** is supported
  for Snowflake and BigQuery.
- **Append** will load new rows in the warehouse into Mixpanel, but will ignore modifications to existing rows
  or rows that have been deleted from the warehouse. We recommend using **Mirror** over **Append** for supported
  warehouses.
- **Full** will reload the entire table to Mixpanel each time it runs rather than tracking changes between runs. Full
  syncs are only supported for Lookup Tables, User Profiles, and Group Profiles.
- **One-Time** will load the data from your warehouse into Mixpanel _once_ with no ability to send incremental
  changer later. This is only recommended where the warehouse is being used as a temporary copy of the data being
  moved to Mixpanel from some other source and the warehouse copy will not be updated later.

### Mirror

Mirror syncs work by having the warehouse compute which rows have been inserted, modified, or deleted and sending this
list of changes to Mixpanel. Change tracking is configured differently depending on the source warehouse. Mirror is
supported for Snowflake, Databricks and BigQuery sources.

<ExtendedTabs urlParam="mirror" urlToItemsMap={mirrorItems}>

<Tabs.Tab>
Mirror takes BigQuery [table snapshots](https://cloud.google.com/bigquery/docs/table-snapshots-intro) and runs queries to compute the
change stream between two snapshot. Snapshots are stored in the `mixpanel` dataset created in [Step 1](#step-1-connect-a-warehouse).

**Considerations when using Mirror with BigQuery:**
- Mirror is not supported on views in BigQuery.
- If two rows in BigQuery are identical across _all_ columns the checksums Mirror computes for each row will be the same
  and Mixpanel will consider them the same row causing only one copy to appear in Mixpanel. We recommend ensuring that one
  of your columns is a unique row ID to avoid this.
- The table snapshots managed by Mixpanel are always created to expire after 21 days. This ensures that the snapshots are
  deleted even if Mixpanel looses access to them unexpectedly. Make sure that the sync does not go longer than 21 days without
  running as each sync run needs access to the previous sync run's snapshot. (Under normal conditions Mirror maintains only one
  snapshot per-sync and removes the older run's snapshot as soon as it has been used by the subsequent sync run).

**How changes are detected:**

Changed rows are detected by checksumming the values of all columns except trailing NULL-valued columns. For example in the following table
would use these per-row checksums:

| ID | Song Name | Artist | Genre | **Computed checksum** |
| --- | --- | --- | --- | --- |
| 12345 | One Dance | Drake | NULL | `CHECKSUM(12345, 'One Dance', 'Drake')` |
| 45678 | Voyager | Daft Punk | Electronic | `CHECKSUM(45678, 'Voyager', 'Daft Punk', 'Electronic')` |
| 83921 | NULL | NULL | Classical | `CHECKSUM(83921, NULL, NULL, 'Classical')` |

Trailing NULL-values are excluded from the checksum to ensure that adding new columns does not change the checksum
of existing rows. For example if a new column is added to the example table:

```jsx
ALTER TABLE songs ADD COLUMN Tag STRING NULL;
```

It would not change the computed checksums:

| ID | Song Name | Artist | Genre | Tag | Computed checksum |
| --- | --- | --- | --- | --- | --- |
| 12345 | One Dance | Drake | NULL | NULL | `CHECKSUM(12345, 'One Dance', 'Drake')` |
| 45678 | Voyager | Daft Punk | Electronic | NULL | `CHECKSUM(45678, 'Voyager', 'Daft Punk', 'Electronic')` |
| 83921 | NULL | NULL | Classical | NULL | `CHECKSUM(83921, NULL, NULL, 'Classical')` |

Until values are written to the new column:

| ID | Song Name | Artist | Genre | Tag | Computed checksum |
| --- | --- | --- | --- | --- | --- |
| 12345 | One Dance | Drake | NULL | tag1 | `CHECKSUM(12345, 'One Dance', 'Drake', NULL, 'tag1')` |
| 45678 | Voyager | Daft Punk | Electronic | tag2 | `CHECKSUM(45678, 'Voyager', 'Daft Punk', 'Electronic', 'tag2')` |
| 83921 | NULL | NULL | Classical | NULL | `CHECKSUM(83921, NULL, NULL, 'Classical')` |

**Handling schema changes when using Mirror with BigQuery:**

Adding new, default-NULL columns to Mirror-tracked tables/views is fully supported as described in the
previous section.

```jsx
ALTER TABLE <table> ADD COLUMN <column> STRING NULL;
```

We recommend avoiding other types of schema changes on large tables. Other schema changes may cause the
checksum of every row to change, effectively re-sending the entire table to Mixpanel. For example, if we
were to remove the Genre column in the example above the checksum of every row would be different:

| ID | Song Name | Artist | Tag | Computed checksum |
| --- | --- | --- | --- | --- |
| 12345 | One Dance | Drake | tag1 | `CHECKSUM(12345, 'One Dance', 'Drake', 'tag1')` |
| 45678 | Voyager | Daft Punk | tag2 | `CHECKSUM(45678, 'Voyager', 'Daft Punk', 'tag2')` |
| 83921 | NULL | NULL | NULL | `CHECKSUM(83921)` |

**Handling partitioned tables:**

When syncing [time partitioned](https://cloud.google.com/bigquery/docs/partitioned-tables#date_timestamp_partitioned_tables) or
[ingestion-time partitioned](https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time) tables Mirror will use partition
metadata to skip processing partitions that have not changed between sync runs. This will make the computation of the change stream
much more efficient on large partitioned tables where only a small percentage of partitions are update between runs. For example,
in a day-partitioned table with two years of data, where only the last five days of data are normally updated only five partitions
worth of data will be scanned each time the sync runs.

</Tabs.Tab>

<Tabs.Tab>
Mirror uses [Snowflake Streams](https://docs.snowflake.com/en/user-guide/streams-intro) to track changes to Snowflake
tables or views. The only requirement to use Mirror on a Snowflake table or view is to enable
[change tracking](https://docs.snowflake.com/en/user-guide/streams-manage):
```jsx
ALTER TABLE <table> SET CHANGE_TRACKING = TRUE;
# or
ALTER VIEW <view> SET CHANGE_TRACKING = TRUE;
```
Mixpanel will create and manage the necessary STREAM objects in the `MIXPANEL` schema created in [Step 1](#step-1-connect-a-warehouse).

**Considerations when using Mirror with Snowflake:**
- Snowflake Streams only maintain change tracking metadata for a limited number of days determined by
  the larger of [DATA_RETENTION_TIME_IN_DAYS and MAX_DATA_EXTENSION_TIME_IN_DAYS](https://docs.snowflake.com/en/user-guide/streams-manage#avoiding-stream-staleness)
  (default is 14 days for MAX_DATA_EXTENSION_TIME_IN_DAYS). Make sure that the Mixpanel sync
  does not go longer than this number of days without running. Mixpanel recommends leaving the default of 14
  days to ensure that if Mixpanel loses access to the warehouse unexpectedly (e.g. a credentials change), there
  is time to correct the issue.
- Snowflake Streams do not work if a table is deleted and re-created with the same name. If using a tool like dbt to model
  data in Snowflake make sure to use an [incremental model](https://docs.getdbt.com/docs/build/incremental-models) so that
  dbt does not replace the table each time it runs.
- Snowflake streams doesn't capture changes when a column is deleted or renamed, so deletion of columns won't be synced to Mixpanel.
- Snowflake has specific requirements when using [Streams on Views](https://docs.snowflake.com/en/user-guide/streams-intro#streams-on-views)
  that must be met when using Mirror with views.
- While Snowflake Streams are a very efficient way of tracking changes there are some performance implications
  when using [Streams on VIEWs that contain JOINs](https://docs.snowflake.com/en/user-guide/streams-intro#join-results-behavior).
  If you find yourself needing such a JOIN in event data we recommends considering if syncing the joined data as [User Profiles](#user-profiles),
  [Group Profiles](#group-profiles), or a [Lookup Table](#lookup-tables) would work instead.

**Handling schema changes when using Mirror with Snowflake:**
Adding new, default-NULL columns to Mirror-tracked tables/views is fully supported.
```jsx
ALTER TABLE <table> ADD COLUMN <column> VARCHAR DEFAULT NULL;
```
We recommend avoiding other types of schema changes. Snowflake streams only reflect changes to tables from DML statements. DDL statements
that logically modify data (e.g. adding new columns with default values, dropping existing columns, or renaming columns) will be reflected
in future data sent to Mixpanel but the Stream will not update historical data with changes caused by DDL statements.
</Tabs.Tab>

<Tabs.Tab>
Mirror uses Databricks [Change Data Feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html) to track changes to Databricks tables.
The only requirement to use Mirror on a Databricks table is to enable [change data feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html#enable-change-data-feed) on those entities.
```jsx
ALTER TABLE <table> SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
```

**Considerations when using Mirror with Databricks:**
- Databricks Change Data Feed only mantains change history for a limited number of days determined by [delta.logRetentionDuration](https://docs.databricks.com/en/delta/history.html#retrieve-delta-table-history) (default is 30 days). Make sure that the Mixpanel sync does not go longer 
than this number of days without running. Mixpanel recommends leaving the default of 30 days to ensure that if Mixpanel loses access to the warehouse unexpectedly (e.g. a credentials change), there is time to correct the issue.
- Databricks Change Data Feed does not work if a table is deleted and re-created with the same name. If using a tool like dbt to model
  data in Databricks make sure to use an [incremental model](https://docs.getdbt.com/docs/build/incremental-models) so that
  dbt does not replace the table each time it runs.
- While Databricks Change Data Feed works with adding new columns to the table or view, there are certain limitations when it comes to dropping columns or renaming columns
which uses [column mappings](https://docs.databricks.com/en/delta/delta-change-data-feed.html#change-data-feed-limitations-for-tables-with-column-mapping-enabled). In such scenarios Mixpanel, recommends deleting the sync (along with the data) and re-creating the sync.

</Tabs.Tab>

</ExtendedTabs>

---

### Append
Append syncs require an Insert Time column in your table. Mixpanel remembers the maximum Insert Time it saw in the previous run of the sync and looks for only rows that have an Insert Time greater than that. This is useful and efficient for append-only tables (usually events) that have a column indicating when the data was appended.

Each time an Append sync runs, it will query the source table with a `WHERE <insert_time_column> > <previous_max_insert_time>` clause. This means that records added with an append time value before the `<previous_max_insert_time>` from the previous run can be missed (not imported) as they would be considered already ingested. The `<insert_time_column>` value should always reflect when the value was made available for Mixpanel to query and ingest.

**Considerations when using Append with large BigQuery tables:**

In an un-partitioned BigQuery table, the `<insert_time_column>` filtering results in a full scan of all data in the source table each time the sync runs. To minimize
BigQuery costs we recommend
[partitioning the source table by the `<insert_time_column>`](https://cloud.google.com/bigquery/docs/partitioned-tables#date_timestamp_partitioned_tables).
Doing so will ensure that each incremental sync run [only scans the most recent partitions](https://cloud.google.com/bigquery/docs/partitioned-tables).

To understand the potential savings consider a 100 GB source table with 100 days of data (approximately 1 GB of data per day):
- If this table is not partitioned and is synced daily the Append sync will scan the whole table (100 GB of data) each time it runs, or 3,000 GB of data per month.
- If this table is partitioned by day and is synced daily with an Append sync the Append sync only scan the current day and previous day's partitions (2 GB of data) each time it runs, or 60 GB of data per day, a 50x improvement over the un-partitioned table.

---

### Full

Full syncs periodically make a snapshot of the source table and sync it entirely to Mixpanel. If a row has new properties in your warehouse, the corresponding profile in Mixpanel will be overridden with those new properties. This mode is available for all tables except events.


## Sync Frequency
Mixpanel offers a variety of sync frequency options to cater to different data integration needs. These options allow you to choose how often your data is synchronized from your data warehouse to Mixpanel, ensuring your data is up-to-date and accurate.

### Standard Sync Frequency Options
* Hourly: Data is synchronized every hour, providing near real-time updates to your Mixpanel project.
* Daily: Data synchronization occurs once a day, ideal for daily reporting and analytics.
* Weekly: Data is synchronized once a week, suitable for less frequent reporting needs.

### Advanced Sync Frequency Option: Trigger via API
For more advanced synchronization needs, Mixpanel offers the ability to trigger syncs via API. This option generates a PUT URL that customers can use in their code to orchestrate Mixpanel sync jobs with other jobs, such as Fivetran pipelines or dbt jobs. By using this API trigger option, you can ensure 100% accuracy by aligning Mixpanel syncs with other critical data operations.

To use the API trigger option:

1. Select Advanced>Trigger Via API under Sync Frequency in the table sync creation UI.
2. After creating the sync, we will generate a PUT URL for you.
3. Integrate this URL into your existing workflows or scripts.
4. Authenticate the request with a Mixpanel Service Account. More information on setting up and using Mixpanel Service Accounts can be found [here](https://developer.mixpanel.com/reference/service-accounts).
5. Trigger the sync job programmatically, ensuring it runs in coordination with other data processes.

This flexibility allows you to maintain precise control over when and how your data is updated in Mixpanel, ensuring your analytics are always based on the latest information.

Note: If your table sync is set up with Mirror mode, you will need to run a sync job at least every 2 weeks to ensure our snapshots do not get deleted. We rate limit the number of syncs via API to 5 per hour.



## FAQ

### What tables are valuable to load into Mixpanel?

Anything that is event-based (has a user_id and timestamp) and that you want to analyze in Mixpanel. Examples, by data source are:
* CRM: Opportunity Created, Opportunity Closed
* Support: Ticket Created, Ticket Closed
* Billing: Subscription Created, Subscription Upgraded, Subscription Canceled, Payment Made
* Application Database: Sign-up, Purchased Item, Invited Teammate

We also recommend loading your user and account tables, to enrich events with demographic attributes about the users and accounts who performed them.

### How fast do syncs transfer data?

Syncs have a throughput of ~30K events+updates/second or ~100M events+updates/hour.

### What is the best way to start bringing in event data?

We recommend starting with a subset of data in a materialized view to test the import process. This allows you to ensure that relevant columns are correctly formatted and the data appears as expected in Mixpanel. Once the data is imported, run a few reports to verify that you can accurately gain insight into your team’s KPIs with the way your data is formatted.

After validating your use case, navigate to the imported table and select "Delete Import" to hard delete the subset data. This step ensures that you can then import the entire table without worrying about duplicate data.

### I already track data to Mixpanel via SDK or CDP, can I still use Warehouse Connectors?

Yes! You can send some events (eg: web and app data) directly via our SDKs and send other data (eg: user profiles from CRM or logs from your backend) from your warehouse and analyze them together in Mixpanel.

### How do I filter for events coming to mixpanel via Warehouse Connector Sync in my reports?

We add couple of hidden properties `$warehouse_import_id` and `$warehouse_type` on every event ingested through warehouse connectors. You can add filters and breakdowns on that property in any Mixpanel report. You can find the Warehouse import id of a sync in Sync History tab shown as `Mixpanel Import Id`.

### How do updates & deletes from Mirror syncs affect my event quota usage?

On an [Events billing plan](/docs/pricing) your event quota is consumed by:

- **Monthly Event Volume:** The number of events in all of your projects at the end of each month. Updating an existing event using Mirror will not affect your monthly event volume. Deleting an existing event using Mirror will decrease your monthly event volume by one.
- **Mirror updates and deletes:** Updates and deletes from mirror are counted separately from event volume. Each update or delete is counted as one event towards billing for the month they were triggered on, even if the record being updated is for a previous month. You can see a breakdown your event quota consumption by "event volume" vs "updates and deletes" on your [organization billing page](https://mixpanel.com/report/settings/%23org%2F%24org_id%24%2Fplan).

You can see how much of your quota is being consumed by each warehouse connector in the detailed [data usage view](https://mixpanel.com/report/settings/%23org%2F%24org_id%24%2Fplan%2Fdetail%2Fevents) for your organization.

On an [MTU billing plan](/docs/pricing/legacy-mtu-billing) updates and deletes from Mirror do not impact [MTU calculations](/docs/pricing/legacy-mtu-billing#mtu-calculation), however updates and deletes are counted as events for the purposes of the [MTU guardrail](/docs/pricing/legacy-mtu-billing#mtu-guardrail) computation.

### What will be the cost impact of this on my DWH?

The DWH cost of using a warehouse connector will vary based on the source warehouse and sync type used. Our connectors use warehouse-specific change tracking to compute modified rows in the warehouse and send only changed data to Mixpanel.

There are 3 aspects of DWH cost: network egress, storage, and compute.
* **Network Egress**: All data is transferred using gzip compression. Assuming an egress rate of \$0.08 per GB and 100 compressed bytes per event, this is a cost of less than \$0.01 per million events. [Mirror](#mirror) and [Append](#append) syncs will only transfer new or modified rows each time they run. [Full](#full) syncs will transfer all rows every time they run. We recommend using Full syncs only for small tables and running them less frequently.
* **Storage**: [Append](#append) and [Full](#full) syncs do not store any additional data in your warehouse, so there are no extra storage costs. [Mirror](#mirror) tracks changes using warehouse-specific functionality that can affect warehouse storage costs:
  * Snowflake: Mirror uses [Snowflake Streams](https://docs.snowflake.com/en/user-guide/streams-intro). Snowflake Streams will [retain historical data](https://docs.snowflake.com/en/user-guide/streams-intro#billing-for-streams) until it is consumed from the stream. As long as the warehouse connector runs regularly data will be consumed regularly and only retained between runs.
  * BigQuery: Mirror uses [table snapshots](https://cloud.google.com/bigquery/docs/table-snapshots-intro). Mirror keeps one snapshot per table to track the contents of the table from the last run. BigQuery table snapshots have no cost when they are first created as they share the underlying storage with the source table, however as the source table changes [the cost of storing changes is attributed to the table snapshot](https://cloud.google.com/bigquery/docs/table-snapshots-intro#storage_costs). Each time the connector runs the current snapshot is replaced with a new snapshot of the latest state of the table. The storage cost is the amount of changes being tracked between the snapshot and source table between runs.
  * Databricks: Mirror uses Databricks [Change Data Feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html) and all the changes are retained in databricks for the [delta.logRetentionDuration](https://docs.databricks.com/en/delta/history.html#retrieve-delta-table-history). Configure that window accordingly to keep storage costs low.
* **Compute**:
  * Mirror on Snowflake: [Snowflake Streams](https://docs.snowflake.com/en/user-guide/streams-intro) natively track changes, the compute cost of querying for these changes is normally proportional to the amount of changed data.
  * Mirror on BigQuery: Each time the connector runs it checksums all rows in the source table and compares them to a [table snapshot](https://cloud.google.com/bigquery/docs/table-snapshots-intro) from the previous run. For large tables we highly recommend [partitioning](https://cloud.google.com/bigquery/docs/partitioned-tables) the source table. When the source table is partitioned the connector will skip checksumming any partitions which have not been modified since the last run. For more details see the BigQuery-specific instructions in [Mirror](#mirror).
  * Mirror on Databricks: Databricks [Change Data Feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html) natively tracks changes to the tables or views, the compute cost of querying these changes is normally proportional to the amount of changed data. Mixpanel reccomends using a smaller compute cluster and setting Auto Terminate after 10 mins of idle time on the compute cluster.
  * Append: All Append syncs run a query filtered on `insert_time_column > [last-run-time]`, the compute cost is the cost of this query. Partitioning or clustering based on `insert_time_column` will greatly improve the performance of this query.
  * Full: Full syncs are always a full table scan of the source table to export it.

### How can I get help setting up a warehouse connector?

[Reach out](https://mixpanel.com/contact-us/sales/) to our team — we’re happy to walk you through the set up. If you bring a data engineer who has credentials to access your warehouse, it takes < 10 minutes to get up and running.
